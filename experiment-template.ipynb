{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dependencies\n",
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# extra code – the next 5 lines define the default font sizes\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Introduce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df = pd.read_csv('data.csv',sep=';')\n",
    "read_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Feature Description\n",
    "list of features and their meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = read_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.hist(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot_features = ['<FEAT_1>', '<FEAT_N>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis_df = read_df[box_plot_features]\n",
    "red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\n",
    "\n",
    "fig, axs = plt.subplots(1, len(analysis_df.columns), figsize=(20,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.boxplot(analysis_df.iloc[:,i], flierprops=red_circle)\n",
    "    ax.set_title(analysis_df.columns[i], fontsize=20, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_df = read_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = '<TARGET>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.2\n",
    "y = test_train_df.drop(target_prediction)\n",
    "# todo: compare holdout to cross validtion\n",
    "#   - Pay attention to your splits and settings\n",
    "#       Are there differences? Why? In which metrics? What could have caused it?\n",
    "#   - Compare/document changes in runtime behaviour with the changing eg. dataset size\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_train_df, y, test_size=test_size, random_state=42)\n",
    "(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = X_train.copy()\n",
    "correlation_df[target_prediction] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df.corr(numeric_only=True)[target_prediction].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(correlation_df, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_combination_df = X_train.copy()\n",
    "feature_combination_df[target_prediction] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create N new feature from existing ones\n",
    "correlation_df['<NEW_FEATURE>'] = correlation_df['<OLD_FEATURE_1>']/correlation_df['<OLD_FEATURE_1>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df.corr(numeric_only=True)[target_prediction].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preproccessing & Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# todo: prepare data (missing values, outliers, scaling, encoding, etc.)\n",
    "preprocessor = Pipeline([\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "nan_and_target = [target_prediction, '<NOT_NUMERIC>']\n",
    "preprocessing_df_num = preprocessing_df.drop(columns=nan_and_target)\n",
    "piped_preprocessing_df = preprocessor.fit_transform(preprocessing_df_num)\n",
    "piped_preprocessing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# todo: implement proper preprocessing for each dataset (missing values, outliers, scaling, encoding, etc.)\n",
    "pipeline = preprocessor\n",
    "X_train_scaled = pipeline.fit_transform(X_train)\n",
    "X_test_scaled = pipeline.transform(X_test)\n",
    "\n",
    "# todo: pick only 3 classifiers, but make sure that \n",
    "#   you choose from at least two different \"types\"/\"paradigms\"\n",
    "#   eg do not choose 3 tree-based classifiers, or 3 NN based classifiers, or 3 ensembles, ...\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "}\n",
    "# todo: Run classifiers, and Experiments with:\n",
    "#   - Different classifiers and your datasets\n",
    "#   - Different parameter settings (= several results per classifier per dataset, not only random/best)\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate accuracy and perform cross-validation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # todo: compare holdout to cross validtion\n",
    "    #   - Pay attention to your splits and settings\n",
    "    #       Are there differences? Why? In which metrics? What could have caused it?\n",
    "    #   - Compare/document changes in runtime behaviour with the changing eg. dataset size\n",
    "    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"cv_mean\": np.mean(cv_scores),\n",
    "        \"cv_std\": np.std(cv_scores)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Test Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"Cross-validation: {result['cv_mean']          :.4f} (+/- {result['cv_std']*2:.4f})\")\n",
    "\n",
    "# todo: Can you identify any patterns/trends?\n",
    "#   - Which methods work well and which did not, is there e.g. one method\n",
    "#       outperforming the others on all datasets?\n",
    "#   - How do the results change when preprocessing strategies change? How sensitive\n",
    "#       is an algorithm to parameter settings?\n",
    "#   - Are there differences across the datasets? Design your experiments so that you\n",
    "#       can investigate the influence of single parameters.\n",
    "# Compare accuracies\n",
    "best_classifier = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"\\nBest Classifier: {best_classifier} with accuracy {\n",
    "      results[best_classifier]['accuracy']:.4f}\")\n",
    "\n",
    "# todo: Evaluate and analyse the performance (primarily effectiveness, \n",
    "#   but also provide basic details on efficiency):\n",
    "#   - Choose suitable, multiple performance measures\n",
    "#   - Make valid comparisons (among the classifiers, across your datasets, parameters,\n",
    "#       preprocessing effects...)\n",
    "#   - (How) can you improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise your results in tables, figures! <br>\n",
    "Document your findings, issues in your report <br>\n",
    "Upload your best results to Kaggle competition (more information below) <br>\n",
    "You do not need to implement the algorithms, rely on libraries/modules <br>\n",
    "- Code just for loading data, pre-processing, running configurations, processing/aggregating results, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Grading key points:\n",
    "- datasets & classifiers description/choice reasoning, preprocessing\n",
    "- classification experiments\n",
    "- analysis of results, summary, interesting findings\n",
    "- submission package & report(formal requirements, clarity, structure)\n",
    "\n",
    "Keep in mind that the grading categories are dependent on each other (e.g. if you do not use preprocessing when\n",
    "needed, your classification and overall analysis will suffer) <br>\n",
    "Your methodology and reasoning are more important for grading than just achieving the highest e.g. accuracy when\n",
    "performing classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Pointers for your project\n",
    "Apply the knowledge from the lectures <br>\n",
    "Document the whole process <br>\n",
    "Carefully design your experiments:\n",
    "- work out your experiment design together as a group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important points:\n",
    "- Explain your choice of datasets, introduce them, their characteristics\n",
    "- Briefly describe the preprocessing steps and argue why you chose them\n",
    "    - Evaluate their impact on the results (mainly scaling)\n",
    "- Explain your choice of classifiers, describe their characteristics\n",
    "    - there is no need to give lengthy explanation about how a classifier works (do not repeat\n",
    "what you heard in the lecture)\n",
    "- Argue on your choice of performance measures\n",
    "    - Think and find multiple, suitable measures, argue why you chose them (why are\n",
    "they necessary, what do they measure/tell us about the performance), and if they\n",
    "are sufficient\n",
    "- In the report, include a paragraph briefly describing the steps you took to ensure\n",
    "that the performance of the classifiers can be compared (think if the\n",
    "comparison makes sense & research what needs to be fulfilled in order to e.g. compare\n",
    "the performance of multiple classifiers on one dataset, how to compare the impact of\n",
    "parameter changes etc.)\n",
    "- Discuss your experimental results, compare them using tables and figures\n",
    "- Provide an aggregated comparison of your results as well - i.e. a big table of the\n",
    "best settings and results for all combinations (and a\n",
    "summary/findings/conclusions!)\n",
    "    - The idea is to extract knowledge from your results, not just list everything without\n",
    "explanations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
