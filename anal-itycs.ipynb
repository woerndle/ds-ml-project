{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import typing\n",
    "import sklearn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fix_model_name_formatting(folder_path):\n",
    "    \"\"\"\n",
    "    Fix the formatting in RF model JSON files by adding a comma between min_samples_leaf and bootstrap.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing JSON files\n",
    "    \"\"\"\n",
    "    # Find all RF JSON files\n",
    "    json_files = glob.glob(os.path.join(folder_path, \"**/*RF*.json\"), recursive=True)\n",
    "    \n",
    "    # Initialize counters\n",
    "    fixed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    for file_path in tqdm(json_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Read the JSON file\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check if this is an RF model file with the formatting issue\n",
    "            if ('model_name' in data and \n",
    "                'RF' in data['model_name'] and \n",
    "                'min_samples_leaf' in data['model_name'] and \n",
    "                'bootstrap' in data['model_name']):\n",
    "                \n",
    "                # Fix the formatting\n",
    "                model_name = data['model_name']\n",
    "                if ' bootstrap=' in model_name:\n",
    "                    # Add comma before bootstrap\n",
    "                    fixed_name = model_name.replace(' bootstrap=', ' ,bootstrap=')\n",
    "                    \n",
    "                    # Update the model_name in the data\n",
    "                    data['model_name'] = fixed_name\n",
    "                    \n",
    "                    # Write the corrected data back to the file\n",
    "                    with open(file_path, 'w') as f:\n",
    "                        json.dump(data, f, indent=4)\n",
    "                    \n",
    "                    fixed_count += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_path}: {str(e)}\")\n",
    "            error_count += 1\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Total files processed: {len(json_files)}\")\n",
    "    print(f\"Files fixed: {fixed_count}\")\n",
    "    print(f\"Errors encountered: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "folder_path = \"test_output_results\"  # Replace with your folder path\n",
    "fix_model_name_formatting(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#    accuracy  precision_weighted  recall_weighted  f1_weighted  accuracy_std  precision_weighted_std  recall_weighted_std  f1_weighted_std  model_name     dataset_name_name\n",
    "# 0  0.900880         0.938316        0.900880     0.908401      0.021403             0.007927           0.021403         0.018926        RF (n_estimators=20...)  amazon_review\n",
    "# 1  0.915234         0.945123        0.915234     0.925678      0.019876             0.008234           0.019876         0.017654        RF (n_estimators=30...)  amazon_review\n",
    "\n",
    "def read_json_files_to_df(folder_path, dataset_name, model_prefix):\n",
    "    # Get all JSON files that start with the model prefix\n",
    "    json_files = glob.glob(os.path.join(folder_path, f'{model_prefix}*.json'))\n",
    "    \n",
    "    # List to store data from all files\n",
    "    data_list = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data['dataset'] = dataset_name\n",
    "            data_list.append(data)\n",
    "    \n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    return df\n",
    "svm_data = pd.concat([\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'amazon_reviews'),'amazon_reviews','SVM'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'congressional_voting'),'congressional_voting','SVM'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'traffic_prediction'),'traffic_situation','SVM'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'wine_reviews'),'wine_reviews','SVM'),\n",
    "    ])\n",
    "rf_data = pd.concat([\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'amazon_reviews'),'amazon_reviews','RF'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'congressional_voting'),'congressional_voting','RF'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'traffic_prediction'),'traffic_situation','RF'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'wine_reviews'),'wine_reviews','RF'),\n",
    "    ])\n",
    "knn_data = pd.concat([\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'amazon_reviews'),'amazon_reviews','KNN'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'congressional_voting'),'congressional_voting','KNN'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'traffic_prediction'),'traffic_situation','KNN'),\n",
    "    read_json_files_to_df(os.path.join('test_output_results', 'wine_reviews'),'wine_reviews','KNN'),\n",
    "    ])\n",
    "all_model_data = pd.concat([\n",
    "        rf_data,\n",
    "        knn_data,\n",
    "        svm_data\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"accuracy\": 0.9008809429812192,\n",
    "#     \"precision_weighted\": 0.9383161061842419,\n",
    "#     \"recall_weighted\": 0.9008809429812192,\n",
    "#     \"f1_weighted\": 0.9084013514080975,\n",
    "#     \"accuracy_std\": 0.021402960283051446,\n",
    "#     \"precision_weighted_std\": 0.007926814601418997,\n",
    "#     \"recall_weighted_std\": 0.021402960283051446,\n",
    "#     \"f1_weighted_std\": 0.01892636657376924,\n",
    "#     \"model_name\": \"RF (n_estimators=20, max_depth=5, min_samples_split=2, min_samples_leaf=2 bootstrap=False,max_features=sqrt,criterions=entropy,class_weights=balanced_subsample)\"\n",
    "# }\n",
    "def parse_model_parameters(df):\n",
    "    \"\"\"\n",
    "    Parse model parameters from the 'model_name' column and create new columns for each parameter.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'model_name' column\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional columns for each parameter\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    def extract_params(model_name):\n",
    "        # Extract the content between parentheses\n",
    "        params_str = model_name[model_name.find(\"(\")+1:model_name.find(\")\")]\n",
    "        \n",
    "        # Split the parameters\n",
    "        params_list = params_str.split(',')\n",
    "        \n",
    "        # Create a dictionary to store parameters\n",
    "        params_dict = {}\n",
    "        \n",
    "        # Extract model type (RF, SVM, or KNN)\n",
    "        params_dict['model_type'] = model_name.split()[0]\n",
    "        \n",
    "        # Parse each parameter\n",
    "        for param in params_list:\n",
    "            param = param.strip()\n",
    "            if '=' in param:\n",
    "                key, value = param.split('=')\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                \n",
    "                # Convert to appropriate data type\n",
    "                if value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value.isdigit():\n",
    "                    value = int(value)\n",
    "                elif value.replace('.', '').isdigit():\n",
    "                    value = float(value)\n",
    "                \n",
    "                params_dict[key] = value\n",
    "                \n",
    "        return params_dict\n",
    "    \n",
    "    # Apply the parsing function to each row and create a DataFrame\n",
    "    params_df = pd.DataFrame([extract_params(name) for name in df_copy['model_name']])\n",
    "    \n",
    "    # Combine the original DataFrame with the parameters DataFrame\n",
    "    result_df = pd.concat([df_copy, params_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "svm_data = parse_model_parameters(svm_data)\n",
    "rf_data = parse_model_parameters(rf_data)\n",
    "knn_data = parse_model_parameters(knn_data)\n",
    "all_model_data = parse_model_parameters(all_model_data)\n",
    "# Usage example:\n",
    "# df_with_params = parse_model_parameters(your_dataframe)\n",
    "\n",
    "# Example output columns for RF models:\n",
    "# Original columns + [\n",
    "#     'model_type',\n",
    "#     'n_estimators',\n",
    "#     'max_depth',\n",
    "#     'min_samples_split',\n",
    "#     'min_samples_leaf',\n",
    "#     'bootstrap',\n",
    "#     'max_features',\n",
    "#     'criterions',\n",
    "#     'class_weights'\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is called 'svm_data'\n",
    "parsed_df = parse_model_parameters(svm_data)\n",
    "\n",
    "# View the new columns\n",
    "print(parsed_df.columns)\n",
    "\n",
    "# Example queries:\n",
    "# Get all RF models with max_depth=5\n",
    "rf_depth_5 = parsed_df[\n",
    "    (parsed_df['model_type'] == 'RF') & \n",
    "    (parsed_df['max_depth'] == 5)\n",
    "]\n",
    "\n",
    "# Get average accuracy grouped by model_type\n",
    "avg_by_model = parsed_df.groupby('model_type')['accuracy'].mean()\n",
    "\n",
    "# Get best performing configuration for each model type\n",
    "best_configs = parsed_df.sort_values('accuracy', ascending=False).groupby('model_type').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "parameter_maps = {\n",
    "    'RF': ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf'],\n",
    "    'SVM': ['C', 'kernel', 'gamma'],\n",
    "    'KNN': ['n_neighbors', 'weights', 'metric']\n",
    "}\n",
    "def analyze_parameter_impact(df, metric='accuracy', figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Analyze how a specific metric changes with parameter variations for each model type and dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the model results and parameters\n",
    "        metric (str): Metric to analyze (e.g., 'accuracy', 'f1_weighted', etc.)\n",
    "        figsize (tuple): Figure size for plots\n",
    "    \"\"\"\n",
    "    # Get unique model types and datasets\n",
    "    model_types = df['model_type'].unique()\n",
    "    datasets = df['dataset'].unique()\n",
    "    \n",
    "    # Parameter mapping for each model type\n",
    "    parameter_maps = {\n",
    "        'RF': ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf'],\n",
    "        'SVM': ['C', 'kernel', 'gamma'],\n",
    "        'KNN': ['n_neighbors', 'weights', 'metric']\n",
    "    }\n",
    "    \n",
    "    for model in model_types:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Analysis for {model} models\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_df = df[df['model_type'] == model]\n",
    "        parameters = parameter_maps.get(model, [])\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            print(f\"\\nDataset: {dataset}\")\n",
    "            dataset_df = model_df[model_df['dataset'] == dataset]\n",
    "            \n",
    "            if dataset_df.empty:\n",
    "                print(\"No data available for this combination\")\n",
    "                continue\n",
    "            \n",
    "            # Create subplots for each parameter\n",
    "            fig, axes = plt.subplots(nrows=(len(parameters) + 1) // 2, \n",
    "                                   ncols=2, \n",
    "                                   figsize=figsize)\n",
    "            fig.suptitle(f'{model} - {dataset}: Parameter Impact on {metric}')\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, param in enumerate(parameters):\n",
    "                if param in dataset_df.columns:\n",
    "                    # Create parameter vs metric plot\n",
    "                    sns.boxplot(data=dataset_df, x=param, y=metric, ax=axes[idx])\n",
    "                    axes[idx].set_title(f'Impact of {param}')\n",
    "                    axes[idx].set_xlabel(param)\n",
    "                    axes[idx].set_ylabel(metric)\n",
    "                    \n",
    "                    # Add mean line\n",
    "                    means = dataset_df.groupby(param)[metric].mean()\n",
    "                    axes[idx].plot(range(len(means)), means.values, 'r-', label='Mean')\n",
    "                    axes[idx].legend()\n",
    "                    \n",
    "                    # Print statistical summary\n",
    "                    print(f\"\\nParameter: {param}\")\n",
    "                    summary = dataset_df.groupby(param)[metric].agg(['mean', 'std', 'count'])\n",
    "                    print(summary)\n",
    "            \n",
    "            # Remove empty subplots\n",
    "            for idx in range(len(parameters), len(axes)):\n",
    "                fig.delaxes(axes[idx])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def print_best_configurations(df, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Print the best configurations for each model type and dataset combination.\n",
    "    \"\"\"\n",
    "    for model in df['model_type'].unique():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Best configurations for {model}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        model_df = df[df['model_type'] == model]\n",
    "        \n",
    "        for dataset in model_df['dataset'].unique():\n",
    "            print(f\"\\nDataset: {dataset}\")\n",
    "            dataset_df = model_df[model_df['dataset'] == dataset]\n",
    "            \n",
    "            # Get best configuration\n",
    "            best_config = dataset_df.nlargest(1, metric).iloc[0]\n",
    "            print(f\"Best {metric}: {best_config[metric]:.4f}\")\n",
    "            print(\"Parameters:\")\n",
    "            for param in parameter_maps[model]:\n",
    "                if param in best_config:\n",
    "                    print(f\"- {param}: {best_config[param]}\")\n",
    "\n",
    "# Usage example\n",
    "# Assuming your DataFrame is called 'parsed_df' and contains the parsed parameters\n",
    "analyze_parameter_impact(parsed_df, metric='accuracy')\n",
    "print_best_configurations(parsed_df, metric='accuracy')\n",
    "\n",
    "# You can also analyze other metrics\n",
    "# analyze_parameter_impact(parsed_df, metric='f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_correlations(df, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Analyze correlations between parameters and metrics.\n",
    "    \"\"\"\n",
    "    for model in df['model_type'].unique():\n",
    "        model_df = df[df['model_type'] == model]\n",
    "        numeric_params = model_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        for dataset in model_df['dataset'].unique():\n",
    "            dataset_df = model_df[model_df['dataset'] == dataset]\n",
    "            \n",
    "            # Create correlation matrix\n",
    "            corr_matrix = dataset_df[list(numeric_params) + [metric]].corr()\n",
    "            \n",
    "            # Plot correlation heatmap\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "            plt.title(f'Parameter Correlations - {model} - {dataset}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "def analyze_parameter_interactions(df, metric='accuracy', param1=None, param2=None):\n",
    "    \"\"\"\n",
    "    Analyze interactions between two parameters.\n",
    "    \"\"\"\n",
    "    for model in df['model_type'].unique():\n",
    "        model_df = df[df['model_type'] == model]\n",
    "        \n",
    "        if param1 is None or param2 is None:\n",
    "            parameters = parameter_maps[model]\n",
    "            param1, param2 = parameters[0], parameters[1]\n",
    "        \n",
    "        for dataset in model_df['dataset'].unique():\n",
    "            dataset_df = model_df[model_df['dataset'] == dataset]\n",
    "            \n",
    "            # Create interaction plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.scatterplot(data=dataset_df, x=param1, y=param2, hue=metric, size=metric)\n",
    "            plt.title(f'Parameter Interaction - {model} - {dataset}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def analyze_parameter_significance(df, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Perform statistical tests to determine parameter significance.\n",
    "    \"\"\"\n",
    "    for model in df['model_type'].unique():\n",
    "        print(f\"\\nStatistical Analysis for {model}\")\n",
    "        model_df = df[df['model_type'] == model]\n",
    "        parameters = parameter_maps[model]\n",
    "        \n",
    "        for dataset in model_df['dataset'].unique():\n",
    "            print(f\"\\nDataset: {dataset}\")\n",
    "            dataset_df = model_df[model_df['dataset'] == dataset]\n",
    "            \n",
    "            for param in parameters:\n",
    "                if param in dataset_df.columns:\n",
    "                    # Perform one-way ANOVA\n",
    "                    groups = [group[metric].values for name, group in dataset_df.groupby(param)]\n",
    "                    f_stat, p_val = stats.f_oneway(*groups)\n",
    "                    \n",
    "                    print(f\"\\nParameter: {param}\")\n",
    "                    print(f\"F-statistic: {f_stat:.4f}\")\n",
    "                    print(f\"p-value: {p_val:.4f}\")\n",
    "                    print(\"Significant\" if p_val < 0.05 else \"Not significant\")\n",
    "\n",
    "# Usage\n",
    "analyze_parameter_significance(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANAL Y SIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_metric_comparison(df, metrics, test='wilcoxon'):\n",
    "    \"\"\"\n",
    "    Compare different distance metrics for kNN.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing kNN results\n",
    "        metrics: List of distance metrics to compare\n",
    "        test: Statistical test to use ('wilcoxon' or 'friedman')\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create box plots for each metric\n",
    "    sns.boxplot(data=df[df['metric'].isin(metrics)], \n",
    "                x='metric', \n",
    "                y='accuracy', \n",
    "                palette='Set3')\n",
    "    \n",
    "    plt.title('Comparison of Distance Metrics in kNN')\n",
    "    plt.xlabel('Distance Metric')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"\\nStatistical Analysis of Distance Metrics\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Prepare data for statistical testing\n",
    "    metric_groups = [df[df['metric'] == m]['accuracy'].values for m in metrics]\n",
    "    \n",
    "    if test == 'wilcoxon':\n",
    "        # Pairwise Wilcoxon tests\n",
    "        for i in range(len(metrics)):\n",
    "            for j in range(i+1, len(metrics)):\n",
    "                stat, p_val = stats.wilcoxon(metric_groups[i], metric_groups[j])\n",
    "                print(f\"\\n{metrics[i]} vs {metrics[j]}:\")\n",
    "                print(f\"Wilcoxon statistic: {stat:.4f}\")\n",
    "                print(f\"p-value: {p_val:.4f}\")\n",
    "    \n",
    "    elif test == 'friedman':\n",
    "        # Friedman test for all metrics\n",
    "        stat, p_val = friedmanchisquare(*metric_groups)\n",
    "        print(\"\\nFriedman Test Results:\")\n",
    "        print(f\"Statistic: {stat:.4f}\")\n",
    "        print(f\"p-value: {p_val:.4f}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def analyze_kernel_performance(df, kernels, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Analyze performance of different SVM kernels.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing SVM results\n",
    "        kernels: List of kernels to compare\n",
    "        metric: Performance metric to analyze\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Create subplot for each analysis\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Box plots for kernel comparison\n",
    "    sns.boxplot(data=df[df['kernel'].isin(kernels)], \n",
    "                x='kernel', \n",
    "                y=metric, \n",
    "                palette='viridis')\n",
    "    plt.title('Kernel Performance Comparison')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Violin plots for distribution visualization\n",
    "    sns.violinplot(data=df[df['kernel'].isin(kernels)], \n",
    "                  x='kernel', \n",
    "                  y=metric, \n",
    "                  palette='viridis')\n",
    "    plt.title('Kernel Performance Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\nKernel Performance Statistics\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = df[df['kernel'].isin(kernels)].groupby('kernel')[metric].agg(['mean', 'std', 'count'])\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary)\n",
    "    \n",
    "    # ANOVA test\n",
    "    kernel_groups = [df[df['kernel'] == k][metric].values for k in kernels]\n",
    "    f_stat, p_val = stats.f_oneway(*kernel_groups)\n",
    "    print(\"\\nANOVA Test Results:\")\n",
    "    print(f\"F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "def analyze_parameter_scale(df, param, scale='log', metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Analyze parameter performance across different scales.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing model results\n",
    "        param: Parameter to analyze\n",
    "        scale: Scale type ('log' or 'linear')\n",
    "        metric: Performance metric to analyze\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Transform parameter values based on scale\n",
    "    if scale == 'log':\n",
    "        param_values = np.log10(df[param])\n",
    "        plt.xscale('log')\n",
    "    else:\n",
    "        param_values = df[param]\n",
    "    \n",
    "    # Scatter plot with trend line\n",
    "    sns.regplot(x=df[param], \n",
    "                y=df[metric], \n",
    "                scatter_kws={'alpha':0.5}, \n",
    "                line_kws={'color': 'red'})\n",
    "    \n",
    "    plt.title(f'{param} vs {metric} ({scale} scale)')\n",
    "    plt.xlabel(f'{param} value ({scale} scale)')\n",
    "    plt.ylabel(metric)\n",
    "    \n",
    "    # Add correlation analysis\n",
    "    correlation = df[[param, metric]].corr().iloc[0,1]\n",
    "    plt.text(0.05, 0.95, f'Correlation: {correlation:.4f}', \n",
    "             transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Parameter value distribution\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(param_values, bins=30)\n",
    "    plt.title(f'Distribution of {param} values')\n",
    "    plt.xlabel(f'{param} value ({scale} scale)')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_model_comparison(df, models, metrics, statistical_test='friedman'):\n",
    "    \"\"\"\n",
    "    Compare performance across different models.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing all model results\n",
    "        models: List of models to compare\n",
    "        metrics: List of metrics to analyze\n",
    "        statistical_test: Type of statistical test to perform\n",
    "    \"\"\"\n",
    "    n_metrics = len(metrics)\n",
    "    plt.figure(figsize=(15, 5*n_metrics))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(n_metrics, 1, i+1)\n",
    "        \n",
    "        # Create violin plots for model comparison\n",
    "        sns.violinplot(data=df[df['model_type'].isin(models)], \n",
    "                      x='model_type', \n",
    "                      y=metric, \n",
    "                      palette='deep')\n",
    "        \n",
    "        plt.title(f'Model Comparison - {metric}')\n",
    "        plt.xlabel('Model Type')\n",
    "        plt.ylabel(metric)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\nModel Comparison Statistics\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\nMetric: {metric}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary = df[df['model_type'].isin(models)].groupby('model_type')[metric].agg(['mean', 'std', 'count'])\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(summary)\n",
    "        \n",
    "        # Statistical test\n",
    "        if statistical_test == 'friedman':\n",
    "            model_groups = [df[df['model_type'] == m][metric].values for m in models]\n",
    "            stat, p_val = friedmanchisquare(*model_groups)\n",
    "            print(\"\\nFriedman Test Results:\")\n",
    "            print(f\"Statistic: {stat:.4f}\")\n",
    "            print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "def analyze_dataset_impact(df, characteristics, models):\n",
    "    \"\"\"\n",
    "    Analyze how dataset characteristics affect model performance.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing all results\n",
    "        characteristics: List of dataset characteristics to analyze\n",
    "        models: List of models to compare\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5*len(characteristics)))\n",
    "    \n",
    "    for i, char in enumerate(characteristics):\n",
    "        plt.subplot(len(characteristics), 1, i+1)\n",
    "        \n",
    "        # Create scatter plots for each model\n",
    "        for model in models:\n",
    "            model_data = df[df['model_type'] == model]\n",
    "            sns.scatterplot(data=model_data, \n",
    "                          x=char, \n",
    "                          y='accuracy', \n",
    "                          label=model, \n",
    "                          alpha=0.6)\n",
    "            \n",
    "            # Add trend line\n",
    "            sns.regplot(data=model_data, \n",
    "                       x=char, \n",
    "                       y='accuracy', \n",
    "                       scatter=False, \n",
    "                       label=f'{model} trend')\n",
    "        \n",
    "        plt.title(f'Impact of {char} on Model Performance')\n",
    "        plt.xlabel(char)\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(\"\\nCorrelation Analysis\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        model_data = df[df['model_type'] == model]\n",
    "        \n",
    "        for char in characteristics:\n",
    "            correlation = model_data[[char, 'accuracy']].corr().iloc[0,1]\n",
    "            print(f\"Correlation with {char}: {correlation:.4f}\")\n",
    "\n",
    "# Usage examples:\n",
    "# Assuming your DataFrame is called 'results_df'\n",
    "\n",
    "# Compare kNN distance metrics\n",
    "analyze_metric_comparison(\n",
    "    all_model_data[all_model_data['model_type'] == 'KNN'],\n",
    "    metrics=['euclidean', 'manhattan', 'minkowski'],\n",
    "    test='wilcoxon'\n",
    ")\n",
    "\n",
    "# Compare SVM kernels\n",
    "analyze_kernel_performance(\n",
    "    all_model_data[all_model_data['model_type'] == 'SVM'],\n",
    "    kernels=['linear', 'rbf', 'poly'],\n",
    "    metric='accuracy'\n",
    ")\n",
    "\n",
    "# Analyze SVM C parameter\n",
    "analyze_parameter_scale(\n",
    "    all_model_data[all_model_data['model_type'] == 'SVM'],\n",
    "    param='C',\n",
    "    scale='log',\n",
    "    metric='accuracy'\n",
    ")\n",
    "\n",
    "# Compare all models\n",
    "analyze_model_comparison(\n",
    "    all_model_data,\n",
    "    models=['RF', 'KNN', 'SVM'],\n",
    "    metrics=['accuracy', 'f1_weighted'],\n",
    "    statistical_test='friedman'\n",
    ")\n",
    "\n",
    "# Analyze dataset impact\n",
    "analyze_dataset_impact(\n",
    "    all_model_data,\n",
    "    characteristics=['n_samples', 'n_features', 'class_balance'],\n",
    "    models=['RF', 'KNN', 'SVM']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters to analyze for RF:\n",
    "rf_params = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']\n",
    "\n",
    "# Correlation Analysis\n",
    "analyze_parameter_correlations(\n",
    "    rf_data,  # DataFrame filtered for RF models\n",
    "    metric='accuracy',\n",
    "    params=rf_params  # Focus on these specific parameters\n",
    ")\n",
    "\n",
    "# Significance Tests for RF\n",
    "# 1. ANOVA test for numerical parameters\n",
    "analyze_parameter_significance(\n",
    "    rf_data, \n",
    "    params=['n_estimators', 'max_depth'],\n",
    "    metric='accuracy'\n",
    ")\n",
    "\n",
    "# 2. Chi-square test for categorical parameters (like criterion)\n",
    "# 3. Friedman test for comparing different parameter combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters for kNN:\n",
    "knn_params = ['n_neighbors', 'weights', 'metric']\n",
    "\n",
    "# Parameter Impact Analysis\n",
    "analyze_parameter_impact(\n",
    "    knn_data,\n",
    "    metric='accuracy',\n",
    "    focus_params=['n_neighbors'],  # Most important parameter\n",
    "    plot_type='line'  # Better for showing k-value progression\n",
    ")\n",
    "\n",
    "# Non-parametric tests for different distance metrics\n",
    "analyze_metric_comparison(\n",
    "    knn_data,\n",
    "    metrics=['euclidean', 'manhattan', 'minkowski'],\n",
    "    test='wilcoxon'  # Paired comparison of distance metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters for SVM:\n",
    "svm_params = ['C', 'kernel', 'gamma']\n",
    "\n",
    "# Kernel Performance Comparison\n",
    "analyze_kernel_performance(\n",
    "    svm_data,\n",
    "    kernels=['rbf', 'linear', 'poly'],\n",
    "    metric='accuracy'\n",
    ")\n",
    "\n",
    "# Parameter Scale Analysis\n",
    "analyze_parameter_scale(\n",
    "    svm_data,\n",
    "    param='C',\n",
    "    scale='log',  # C and gamma often need log scale analysis\n",
    "    metric='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models across datasets\n",
    "analyze_model_comparison(\n",
    "    all_model_data,\n",
    "    models=['RF', 'KNN', 'SVM'],\n",
    "    metrics=['accuracy', 'f1_weighted'],\n",
    "    statistical_test='friedman'  # Non-parametric test for multiple models\n",
    ")\n",
    "\n",
    "# Dataset characteristic impact\n",
    "analyze_dataset_impact(\n",
    "    all_model_data,\n",
    "    characteristics=['n_samples', 'n_features', 'class_balance'],\n",
    "    models=['RF', 'KNN', 'SVM']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_parameter_plot(df, model_type, param, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Create an advanced visualization for parameter analysis\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Main parameter vs metric plot\n",
    "    sns.boxplot(data=df[df['model_type'] == model_type], \n",
    "                x=param, \n",
    "                y=metric, \n",
    "                color='lightblue')\n",
    "    \n",
    "    # Add trend line\n",
    "    sns.regplot(data=df[df['model_type'] == model_type],\n",
    "                x=param,\n",
    "                y=metric,\n",
    "                scatter=False,\n",
    "                color='red')\n",
    "    \n",
    "    # Add standard deviation bands\n",
    "    param_stats = df[df['model_type'] == model_type].groupby(param)[metric].agg(['mean', 'std'])\n",
    "    plt.fill_between(param_stats.index,\n",
    "                     param_stats['mean'] - param_stats['std'],\n",
    "                     param_stats['mean'] + param_stats['std'],\n",
    "                     alpha=0.2)\n",
    "    \n",
    "    plt.title(f'Impact of {param} on {metric} for {model_type}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Statistical Summary Function\n",
    "def print_statistical_summary(df, model_type, param, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Print comprehensive statistical summary\n",
    "    \"\"\"\n",
    "    model_data = df[df['model_type'] == model_type]\n",
    "    \n",
    "    print(f\"Statistical Summary for {model_type} - {param}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = model_data.groupby(param)[metric].agg(['mean', 'std', 'count'])\n",
    "    print(\"\\nParameter-wise statistics:\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Statistical tests\n",
    "    if model_data[param].nunique() > 2:\n",
    "        # ANOVA for more than 2 groups\n",
    "        f_stat, p_val = stats.f_oneway(*[group[metric].values \n",
    "                                       for name, group in model_data.groupby(param)])\n",
    "        print(\"\\nANOVA Test:\")\n",
    "        print(f\"F-statistic: {f_stat:.4f}\")\n",
    "        print(f\"p-value: {p_val:.4f}\")\n",
    "    else:\n",
    "        # T-test for 2 groups\n",
    "        group1, group2 = [group[metric].values \n",
    "                         for name, group in model_data.groupby(param)]\n",
    "        t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "        print(\"\\nT-test:\")\n",
    "        print(f\"t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"p-value: {p_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
